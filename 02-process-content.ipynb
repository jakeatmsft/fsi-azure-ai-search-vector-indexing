{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "153d560d-a721-426e-9976-913d8bae17cf",
      "metadata": {},
      "source": [
        "# Process Content\n",
        "Purpose is to take content stored in blob storage and process it into a separate container.\n",
        "The new container will consist of json files that have all the data needed to push into Azure AI Search.\n",
        "This json data is stored for potential BCDR and Geo-replication needs so that content does not need to be reprocessed.\n",
        "\n",
        "## Required for this step\n",
        "- Azure Blob Storage (with content)\n",
        "- Azure OpenAI (completions model and Ada-002 embeddings)\n",
        "- Azure Document Intelligence\n",
        "\n",
        "## Important\n",
        "- This demo was done on Ubuntu which uses LibreOffice to do conversion of documents to PDF for a standard processing format\n",
        "- PDFKit is used for converting html to PDF - this may need: sudo apt-get install wkhtmltopdf  \n",
        "- If using Linux run: sudo apt-get install libreoffice\n",
        "  - eg: !lowriter --convert-to pdf marketbulletin021505.doc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c399b70-9d9b-4a12-8f76-437b669cc462",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Import required libraries  \n",
        "import os  \n",
        "import base64\n",
        "from pathlib import Path\n",
        "from shutil import rmtree\n",
        "from requests import get, post\n",
        "import json\n",
        "import time\n",
        "import copy  \n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, generate_blob_sas, BlobSasPermissions  \n",
        "from datetime import datetime, timedelta  \n",
        "import pdfkit\n",
        "from langchain.text_splitter import TokenTextSplitter, MarkdownHeaderTextSplitter\n",
        "import pickle\n",
        "from openai import AzureOpenAI\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed974b8-42bc-43fa-87ce-ecee61abb09c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load the configuration details for the Cognitive Search Service and Azure OpenAI Instance\n",
        "#Credentials should be secured using a more secure method such as Azure KeyVault\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from environment variables\n",
        "config = {\n",
        "    \"cognitive_search_service_name\": os.getenv(\"COGNITIVE_SEARCH_SERVICE_NAME\"),\n",
        "    \"cognitive_search_index_name\": os.getenv(\"COGNITIVE_SEARCH_INDEX_NAME\"),\n",
        "    \"cognitive_search_api_key\": os.getenv(\"COGNITIVE_SEARCH_API_KEY\"),\n",
        "    \"blob_service_name\": os.getenv(\"BLOB_SERVICE_NAME\"),\n",
        "    \"blob_container\": os.getenv(\"BLOB_CONTAINER\"),\n",
        "    \"blob_key\": os.getenv(\"BLOB_KEY\"),\n",
        "    \"openai_api_base\": os.getenv(\"OPENAI_API_BASE\"),\n",
        "    \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "    \"openai_api_version\": os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    \"openai_embedding_model\": os.getenv(\"OPENAI_EMBEDDING_MODEL\"),\n",
        "    \"openai_gpt_model\": os.getenv(\"OPENAI_GPT_MODEL\"),\n",
        "    \"doc_intelligence_endpoint\": os.getenv(\"DOC_INTELLIGENCE_ENDPOINT\"),\n",
        "    \"doc_intelligence_apim_key\": os.getenv(\"DOC_INTELLIGENCE_APIM_KEY\"),\n",
        "    \"data_root_dir\": os.getenv(\"DATA_ROOT_DIR\", \"./data\")\n",
        "}\n",
        "\n",
        "# Azure Blob Storage Config\n",
        "blob_service_name = config[\"blob_service_name\"]\n",
        "blob_container = config[\"blob_container\"]\n",
        "blob_key = config[\"blob_key\"]\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=\" + blob_service_name + \";AccountKey=\" + blob_key + \";EndpointSuffix=core.windows.net\"  \n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)  \n",
        "container_client = blob_service_client.get_container_client(blob_container) \n",
        "\n",
        "#Azure OpenAI\n",
        "api_base = config[\"openai_api_base\"]\n",
        "api_key = config[\"openai_api_key\"]\n",
        "openai_api_version = config[\"openai_api_version\"]\n",
        "embeddings_model = config[\"openai_embedding_model\"]\n",
        "gpt_model = config[\"openai_gpt_model\"] \n",
        "\n",
        "# Doc Intelligence Config\n",
        "di_endpoint = config[\"doc_intelligence_endpoint\"]\n",
        "di_apim_key = config[\"doc_intelligence_apim_key\"]\n",
        "di_headers = {\n",
        "    'Content-Type': 'application/pdf',\n",
        "    'Ocp-Apim-Subscription-Key': di_apim_key,\n",
        "}\n",
        "di_post_url = di_endpoint + \"documentintelligence/documentModels/prebuilt-layout:analyze?api-version=2023-10-31-preview&stringIndexType=utf16CodeUnit&outputContentFormat=markdown\"\n",
        "\n",
        "# Set a temp directory for downloading pdf's for processing\n",
        "data_root_dir = config[\"data_root_dir\"]\n",
        "tmp_dir = os.path.join(data_root_dir, \"tmp\")\n",
        "pkl_dir = os.path.join(data_root_dir, \"pkl\")\n",
        "json_dir = os.path.join(data_root_dir, \"json\")\n",
        "\n",
        "# Chunking Config\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=52)  \n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "client = AzureOpenAI(\n",
        "    api_version=openai_api_version,\n",
        "    azure_endpoint=api_base,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "print ('Temp Dir:', tmp_dir)\n",
        "print ('Pickle Dir:', pkl_dir)\n",
        "print ('JSON Dir:', json_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87aad72a-2f4d-4104-be86-07eb578b916e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_embeddings(text):\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=embeddings_model\n",
        "    )\n",
        "    return json.loads(response.model_dump_json())[\"data\"][0]['embedding']\n",
        "\n",
        "# Create a title based on a supplied set of text\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_title(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=gpt_model, \n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Assistant who creates succint titles for content.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# reset output dir\n",
        "def reset_dir(dir):\n",
        "    processed_path = Path(dir)\n",
        "    if processed_path.exists():\n",
        "        rmtree(processed_path)\n",
        "    processed_path.mkdir(parents=True)\n",
        "\n",
        "# Get all files in dir\n",
        "def get_files_in_dir(in_dir):\n",
        "    return [os.path.join(dp, f) for dp, dn, filenames in os.walk(in_dir) for f in filenames]\n",
        "\n",
        "def base64_encode_string(s):\n",
        "    # encode the string into bytes, then encode it in base64  \n",
        "    encoded = base64.b64encode(s.encode('utf-8'))  \n",
        "    return encoded.decode('utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1c898c-ee71-4500-9106-59db5cc90372",
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate_embeddings('test')[:10]\n",
        "# generate_title('The quick brown fox jumped over the lazy dog.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b634015d-5d1f-4248-9733-40c7c4e53c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories for downloading and processing\n",
        "reset_dir(tmp_dir)\n",
        "reset_dir(pkl_dir)\n",
        "reset_dir(json_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf678d3-9648-499e-b19a-8dc668512daa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and process blobs\n",
        "blob_list = container_client.list_blobs()  \n",
        "\n",
        "# Iterate through each blob  \n",
        "documents = []\n",
        "for blob in blob_list:  \n",
        "    file_type = os.path.splitext(blob.name)[1].lower()\n",
        "    pkl_file = os.path.join(pkl_dir, os.path.basename(blob.name) + '.pkl')\n",
        "\n",
        "\n",
        "    if os.path.exists(pkl_file) == False:\n",
        "        if (file_type == \".pdf\") or (file_type == \".docx\") or (file_type == \".doc\") or (file_type == \".html\") or (file_type == \".htm\"):\n",
        "            print ('Processing', blob.name)\n",
        "            # Create a blob client for the blob  \n",
        "            blob_client = blob_service_client.get_blob_client(blob_container, blob.name)  \n",
        "            local_file = os.path.join(tmp_dir, blob.name)\n",
        "\n",
        "            # Download file locally\n",
        "            print ('Downloading', blob.name, 'to', local_file, '...')\n",
        "            with open(local_file, \"wb\") as download_file:  \n",
        "                download_file.write(blob_client.download_blob().readall())  \n",
        "    \n",
        "            pdf_file = local_file\n",
        "            # Conver file to PDF format (if it is not already PDF)\n",
        "            if file_type != \".pdf\":\n",
        "                print ('Converting file to PDF format...')\n",
        "                pdf_file = os.path.join(tmp_dir, blob.name.split('.')[:len(blob.name.split('.'))-1][0] + '.pdf')\n",
        "                if file_type == \".html\":\n",
        "                    pdfkit.from_file(local_file, pdf_file)\n",
        "                else:\n",
        "                    os.system(\"lowriter --convert-to pdf \" + local_file + \" --outdir \" + tmp_dir) \n",
        "    \n",
        "            print (pdf_file)\n",
        "    \n",
        "            print ('Processing', pdf_file)\n",
        "            with open(pdf_file, \"rb\") as f:\n",
        "                data_bytes = f.read()\n",
        "    \n",
        "    \n",
        "            resp = post(url = di_post_url, data = data_bytes, headers = di_headers)\n",
        "            if resp.status_code != 202:\n",
        "                print(\"POST analyze failed:\\n%s\" % resp.text)\n",
        "                quit()\n",
        "            print(\"POST analyze succeeded:\\n%s\" % resp.headers)\n",
        "            get_url = resp.headers[\"operation-location\"]\n",
        "            \n",
        "            if resp.status_code == 202:\n",
        "                get_url = resp.headers['Operation-Location']\n",
        "                print (get_url)\n",
        "            \n",
        "            n_tries = 10\n",
        "            n_try = 0\n",
        "            wait_sec = 2\n",
        "            processing = True\n",
        "            while processing:\n",
        "                try:\n",
        "                    resp = get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": di_apim_key})\n",
        "                    resp_json = json.loads(resp.text)\n",
        "                    if resp.status_code != 200:\n",
        "                        # print(\"GET Layout results failed:\\n%s\" % resp_json)\n",
        "                        print(\"GET Layout results failed:\\n\")\n",
        "                        processing = False\n",
        "                    elif resp_json[\"status\"] == \"succeeded\":\n",
        "                        # print(\"Layout Analysis succeeded:\\n%s\" % resp_json)\n",
        "                        print(\"Layout Analysis succeeded:\\n\")\n",
        "                        print(\"--------------------------------\")\n",
        "                        processing = False\n",
        "                    elif resp_json[\"status\"] == \"failed\":\n",
        "                        # print(\"Layout Analysis failed:\\n%s\" % resp_json)\n",
        "                        print(\"Layout Analysis failed:\\n\")\n",
        "                        processing = False\n",
        "                    else:\n",
        "                        # Analysis still running. Wait and retry.\n",
        "                        print ('Waiting to complete processing...')\n",
        "                        time.sleep(wait_sec)\n",
        "                except Exception as e:\n",
        "                    msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
        "                    print(msg)\n",
        "                    processing = False\n",
        "            \n",
        "            # Persist the Doc Int Output for further processing\n",
        "            if 'analyzeResult' in resp_json:\n",
        "                with open(pkl_file, 'wb') as pkl_out:\n",
        "                    pickle.dump(resp_json['analyzeResult'], pkl_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            \n",
        "        else:\n",
        "            print ('Skipping - Unsupported file type')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed470446-04f0-4c93-8a4c-4f79c0216ad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the pkl files for processing of JSON files\n",
        "pkl_files = get_files_in_dir(pkl_dir)\n",
        "total_files = len(pkl_files)\n",
        "print ('Total PKL files:', total_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0ddace-6901-44c2-8983-565559d69ce2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the Pickle files which contain the results of the Document Intelligence Analyze Results\n",
        "# We will use the markdown text within this for chunking, etc\n",
        "# The output will be a set of JSON files which be uploaaded to Azure AI Search in the next step\n",
        "# These JSON files can be saved for BCDR purposes so that you do not need to reprocess the original content\n",
        "for pkl_file in pkl_files:\n",
        "    print (pkl_file)\n",
        "    \n",
        "    json_data_base = {}\n",
        "    base_file = os.path.basename(pkl_file)\n",
        "    base_file=base_file[:base_file.rfind('.pkl')]\n",
        "    json_out_file = os.path.join(json_dir, base_file + \".json\")\n",
        "\n",
        "    if os.path.exists(json_out_file) == False:\n",
        "        json_data_base[\"parent_id\"] = base64_encode_string(os.path.basename(pkl_file)[:pkl_file.rfind('.pkl')])\n",
        "        # json_data_base[\"url\"] = download_url\n",
        "        json_data_base[\"file_name\"] = os.path.basename(pkl_file)[:pkl_file.rfind('.pkl')]\n",
        "        json_data_base[\"last_updated\"] = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
        "    \n",
        "        with open(pkl_file, 'rb') as pkl_in:\n",
        "            analyze_result = pickle.load(pkl_in)\n",
        "        content = analyze_result['content']\n",
        "    \n",
        "        md_header_splits = markdown_splitter.split_text(content)\n",
        "        documents = []\n",
        "        section_counter = 0\n",
        "        total_sections = len(md_header_splits)\n",
        "        chunk_id = 0\n",
        "        for s in md_header_splits:\n",
        "            section_counter+=1\n",
        "\n",
        "            section_content = s.page_content\n",
        "            chunks = text_splitter.split_text(section_content)\n",
        "            print ('Processing Section:', section_counter, 'of', total_sections, 'with', len(chunks), 'chunks...')\n",
        "\n",
        "            if chunks != []:\n",
        "                for chunk in chunks:\n",
        "                    json_data = copy.deepcopy(json_data_base)\n",
        "                    json_data[\"chunk_id\"] = str(chunk_id)\n",
        "                    json_data[\"chunk\"] = chunk\n",
        "                    # Ensure title is always a string\n",
        "                    title = generate_title(json_data['chunk'])\n",
        "                    if title is None:\n",
        "                        title = \"Untitled Section\"\n",
        "                    json_data[\"title\"] = title\n",
        "                    chunk_content = \"File Name: \" + base_file + \"\\n\"\n",
        "                    chunk_content += \"Section Title: \" + json_data[\"title\"] + \"\\n\"\n",
        "                    chunk_content += chunk\n",
        "                    json_data[\"vector\"] = generate_embeddings(chunk_content)\n",
        "                    chunk_id+=1\n",
        "                    documents.append(json_data)\n",
        "            else:\n",
        "                print ('No content found for this file')\n",
        "    \n",
        "        with open(json_out_file, \"w\") as j_out:\n",
        "            j_out.write(json.dumps(documents))\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e945379-6d7f-48b6-b3f0-13f3c1509c57",
      "metadata": {},
      "outputs": [],
      "source": [
        "        with open(json_out_file, \"w\") as j_out:\n",
        "            j_out.write(json.dumps(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66af224f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}